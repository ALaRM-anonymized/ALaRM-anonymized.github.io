<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Anonymized project page of ALaRM for review.">
  <!-- <meta name="keywords" content="alarm, hierarchical, rlhf"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ALaRM: Align Language Models via Hierarchical Rewards Modeling</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/alarm-clock.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="dnerf">ALaRM</span>: Align Language Models via Hierarchical Rewards Modeling</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Authors<sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Anonymous Institution</span>
          </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">ALaRM</span> is a reinforcement learning framework for enhancing LLM alignment that:
        <br>
        (1) integrates holistic and aspect-specific rewards hierarchically,
        (2) employs a selection and combination methodology based on reward consistency,
        (3) demonstrates effectiveness in long-form question answering and machine translation.
      </h2>
      <img src="static/images/framework.svg">
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce <span class="dnerf">ALaRM</span>, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences.
          </p>
          <p>
            The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a robust mechanism for improving model alignment.
          </p>
          <p>
            We validate our approach through applications in long-form question answering and machine translation tasks, employing <i>gpt-3.5-turbo-1106</i> for pair-wise comparisons, and demonstrate significant improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better preference alignment.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Motivation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <img src="static/images/training_signals.svg">
          <p>
            Human oversight capabilities are finite. Human-written demonstrations and human preferences over two or more answers become more diffcult due to rapidly growing LLMs. We ask, <b>how to get reliable and scalable supervision signals within limited human oversight capabilities?</b>
          </p>
          <p>
            As shown in this figure, the pretrained policies are first supervised fine-tuned on human-written demonstrations and then trained through RLHF given a holistic reward learned from human-ranked comparisons. The shadowed "noisy" area better aligns with human preference, which is hard to reach for solely a noisy holistic reward. We propose to utilize multiple rewards hierarchically for more accurate and consistent supervision signals and thus guide the policies into the noisy area.
          </p>
        </div>
      </div>
    </div>
    <!--/ Motivation. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Long-form QA -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Results</h2>
        <div class="content has-text-justified">
          <p>
            In this paper, we conduct all experiments using three different seeds, and the results are averaged across these three independent runs. In both long-form question answering and machine translation, we can see <span class="dnerf">ALaRM</span> holds the best under all three different metrics, which proves that <span class="dnerf">ALaRM</span> provides a stronger supervision signal than other methods.
          </p>
          <p>
            <b>Holistc Reward</b>: <a href="https://huggingface.co/openbmb/UltraRM-13b">UltraRM-13B</a>
            <br>
            <b>Aspect-specific Rewards</b>: Factuality for Long-form QA; Grammar Reward by <a href="https://github.com/languagetool-org/languagetool">LanguageTool</a> for MT
            <br>
          </p>
          <img src="static/images/long-form-QA-win-rate.png">
          <img src="static/images/MT-win-rate.png">
          <br><br>
          <p>
            Following tables shows the evaluation results on the test set of the mean values of each reward. We can see that <span class="dnerf">ALaRM</span> leads to significantly higher holistic reward than other methods, meanwhile presenting a comparable factuality rate/grammar error rate.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-two-fifths">
        <div class="content">
          <img src="static/images/long-form-QA-mean.png">
        </div>
      </div>
      <div class="column is-two-fifths">
        <div class="content">
          <img src="static/images/MT-mean.png">
        </div>
      </div>
    </div>
    <!--/ Long-form QA -->
  </div>
</section>

</body>
</html>
