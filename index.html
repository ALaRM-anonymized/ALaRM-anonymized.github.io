<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Anonymized project page of ALaRM for review.">
  <!-- <meta name="keywords" content="alarm, hierarchical, rlhf"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ALaRM: Align Language Models via Hierarchical Rewards Modeling</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/alarm-clock.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="dnerf">ALaRM</span>: Align Language Models via Hierarchical Rewards Modeling</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Authors<sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Anonymous Institution</span>
          </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">ALaRM</span> is a reinforcement learning framework for enhancing LLM alignment that:
        <br>
        (1) integrates holistic and aspect-specific rewards hierarchically,
        (2) employs a selection and combination methodology based on reward consistency,
        (3) demonstrates effectiveness in long-form question answering and machine translation.
      </h2>
      <img src="static/images/framework.svg">
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce <span class="dnerf">ALaRM</span>, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences.
          </p>
          <p>
            The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment.
          </p>
          <p>
            We validate our approach through applications in long-form question answering and machine translation tasks, employing <i>gpt-3.5-turbo</i> for pairwise comparisons, and demonstrate improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Motivation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <img src="static/images/training_signals.svg">
          <p>
            Human oversight capabilities are finite. Demonstrations or preferences get noisy when tasks become complicated. We ask, <b>how to get reliable and scalable supervision signals within limited human oversight capabilities?</b>
          </p>
          <p>
            As shown in this figure, the shadowed "superior area" better aligns with human preference, which is hard to reach for solely a noisy holistic reward. We propose to utilize multiple rewards hierarchically for more accurate and consistent supervision signals.
          </p>
        </div>
      </div>
    </div>
    <!--/ Motivation. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Long-form QA -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Task: Long-form Question Answering</h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-two-fifths">
        <div class="content">
          <img src="static/images/long-form-QA-reward-selection.png">
        </div>
      </div>
      <div class="column is-two-fifths">
        <div class="content">
          <img src="static/images/long-form-QA-mean.png">
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            <li>Through proactive reward selection, we choose the factuality reward for Long-form QA.</li>
            <li>Results show that <span class="dnerf">ALaRM</span> holds the best in both mean rewards and pairwise comparisons.</li>
          </p>

          <img src="static/images/long-form-QA-win-rate.png">
        </div>
      </div>
    </div>
    <!--/ Long-form QA -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- MT -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Task: Machine Translation</h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-two-fifths">
        <div class="content">
          <img src="static/images/MT-reward-selection.png">
        </div>
      </div>
      <div class="column is-two-fifths">
        <div class="content">
          <img src="static/images/MT-mean.png">
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            <li>The same as in long-form QA, we choose the grammar reward by the results of reward selection.</li>
            <li>Again, <span class="dnerf">ALaRM</span> wins in pairwise comparisons and presents leading performance in mean rewards.</li>
          </p>

          <img src="static/images/MT-win-rate.png">
        </div>
      </div>
    </div>
    <!--/ MT -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Ablation Study. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ablation Study</h2>
        <div class="content has-text-justified">
          <div style="text-align: center;">
            <img src="static/images/ablation.png" width="60%">
          </div>
          <p>
            While the main results should support the significance of <b>Combination</b> and <b>Hierarchical Structure</b>, we conduct extensive experiments to find out how <b>Reward Selection</b> affects <span class="dnerf">ALaRM</span>.
          </p>
          <p>
            As shown in the table, the proactively selected rewards present leading performance evaluated by both the holistic reward and <i>gpt-3.5-turbo</i>, demonstrating the effectiveness of <b>Reward Selection</b>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Ablation Study. -->
  </div>
</section>

</body>
</html>
